{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f72f597f",
   "metadata": {},
   "source": [
    "# 数据操作\n",
    "\n",
    "首先介绍张量（tenser），其实就是 $n$ 维的 array，但是在深度学习框架中叫张量类。\n",
    "\n",
    "深度学习框架又比 Numpy 的 ndarray 多一些重要功能：\n",
    "\n",
    "- 首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算； \n",
    "- 其次，张量类支持自动微分。这些功能使得张量类更适合深度学习。如果没有特殊说明，本书中所说的张量均指的是张量类的实例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8a970a",
   "metadata": {},
   "source": [
    "## 张量入门\n",
    "\n",
    "和 Numpy 差不多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cab9f42d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b519e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape # size of the tenser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ee5cdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel() # number of the element in a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6614528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(-1, 4) # same as numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be287f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2, 3, 4) # same as numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44bd9b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8137, -1.2267, -0.7105,  0.3359],\n",
       "        [-0.3480,  0.3728,  0.7156,  0.5010],\n",
       "        [-0.7090, -0.2571,  0.2815,  0.3627]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 4) # random: standard Guassion distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c80e2834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4],\n",
       "        [5, 6, 7, 8]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be29d46",
   "metadata": {},
   "source": [
    "注意下这个：连结（concatenate）两个张量：\n",
    "\n",
    " - `dim=0` 就是沿着上下方向\n",
    " - `dim=1` 就是沿着左右方向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daa8a5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "\n",
    "torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)\n",
    "\n",
    "# From the result we can see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a2256f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum() # Note that the result is still a tenser type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1853a5",
   "metadata": {},
   "source": [
    "### 广播机制\n",
    "\n",
    "Numpy 里没认真看，在这看看。\n",
    "\n",
    "对于尺寸相同的张量，我们都可以进行逐元素操作。但是对于尺寸不同的呢？广播机制在某些情况下也可以。这种机制的工作方式如下：首先，通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状。其次，对生成的数组执行按元素操作。\n",
    "\n",
    "在大多数情况下，我们将沿着数组中**长度为1的轴**进行广播，如下例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8df0b9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((-1, 1))\n",
    "b = torch.arange(2).reshape((1, -1))\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da3035af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f2945",
   "metadata": {},
   "source": [
    "沿着长度为 1 的轴进行广播。也就是就将单维复制为多维：\n",
    "\n",
    "`a` 被复制为 `[[0, 0], [1, 1], [2, 2]]`，也就是由一列复制为了两列\n",
    "\n",
    "`b` 被复制为 `[[0, 1], [0, 1], [0, 1]]`，也就是一行复制为了三行\n",
    "\n",
    "但是这个操作并没有改变原来的张量，而是为了计算临时改变的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7105c46",
   "metadata": {},
   "source": [
    "## 节省内存\n",
    "\n",
    "运行一些操作可能会导致为新结果分配内存。例如，如果我们用 `Y = X + Y`，我们将取消引用 `Y` 指向的张量，而是指向新分配的内存处的张量。\n",
    "\n",
    "可以实验演示，确实是这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a33ef16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152f8169",
   "metadata": {},
   "source": [
    "在深度学习中，我们认为这是不好的，因为数据常常会更新，而且数据量很大，这么做可能会导致一些错误。\n",
    "\n",
    "所以我们的解决方法就是执行原地操作，方法也很简单，使用切片表示法或者 `+=` 此类运算符即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f64e0cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1\n",
    "before = id(X)\n",
    "X[:] = X + Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3293b32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 2\n",
    "before = id(X)\n",
    "X += Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b76c1f7",
   "metadata": {},
   "source": [
    "## 转化为其他对象\n",
    "\n",
    "事实上 Numpy 数组与 Pytorch 张量共享底层，所以二者基本混着用没什么问题，而且也有内建函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04d4c2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.numpy()\n",
    "B = torch.tensor(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6622b5f5",
   "metadata": {},
   "source": [
    "使用其中的常数也很容易："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f16640de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.7000]), 3.700000047683716)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = torch.tensor([3.7])\n",
    "aa, aa.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c618a22a",
   "metadata": {},
   "source": [
    "## 练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dc8fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
