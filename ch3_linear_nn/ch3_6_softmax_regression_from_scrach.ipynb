{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax回归的从零开始实现\n",
    "\n",
    "softmax 回归也是重要的基础，因此你应该知道实现 softmax 回归的细节。还是使用 Fashion-mnist 数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython import display\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型参数\n",
    "\n",
    "目前我们认为 28*28 图像展平为一维，每个像素都为一个特征，一张图片共有 784 个特征。后面在学习如何处理多维图像的特征\n",
    "\n",
    "由于有十个种类，所以网络设置为具有 10 个输出。所以当网络只有两层的时候，权值矩阵的尺寸为 $(784, 10)$，偏置也组成一个 10 维向量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\n",
    "b = torch.zeros(num_outputs, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义 Softmax 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    partition = X_exp.sum(1, keepdim=True)\n",
    "    return X_exp / partition # 这里应用了广播机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return Softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "一个问题吧，例如我们用的 Fashion-mnist 数据集，它的标注只有一个数字，例如 3 代表这是第四类、0 代表这是第一类。但是，我们经过网络计算的出来的值是有 10 个输出的。怎么把标注这一个数组变成类似于 `[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]` 这样的代表一类正确的形式然后再计算损失函数，然后计算出每个输出上的 loss 作为一个向量呢？\n",
    "\n",
    "$$\n",
    "l(\\hat{y}, y) = - \\sum_{j=1}^{q} y_{j} \\log{\\hat{y_{j}}}\n",
    "$$\n",
    "\n",
    "我们的方法是这样，每次看都仔细读读吧！\n",
    "\n",
    "（[这里](https://zh-v2.d2l.ai/chapter_linear-networks/softmax-regression-scratch.html#id4)有一个引论）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cross_entropy(y_hat, y):\n",
    "    return -torch.log(y_hat[range(len(y_hat)), y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('dl_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0559c7b697746d8ada537ecaf8430efdacd6ab4f7a5ef8d73b5846265590c7b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
