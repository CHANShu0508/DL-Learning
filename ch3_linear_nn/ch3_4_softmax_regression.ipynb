{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax 回归\n",
    "\n",
    "其实就是由纯回归问题转变为分类问题\n",
    "\n",
    "## 分类问题\n",
    "\n",
    "我们从一个图像分类问题开始。\n",
    "假设每次输入是一个$2\\times2$的灰度图像。\n",
    "我们可以用一个标量表示每个像素值，每个图像对应四个特征$x_1, x_2, x_3, x_4$。\n",
    "此外，假设每个图像属于类别“猫”，“鸡”和“狗”中的一个。\n",
    "\n",
    "接下来，我们要选择如何表示标签。\n",
    "我们有两个明显的选择：最直接的想法是选择$y \\in \\{1, 2, 3\\}$，\n",
    "其中整数分别代表$\\{\\text{狗}, \\text{猫}, \\text{鸡}\\}$。\n",
    "这是在计算机上存储此类信息的有效方法。\n",
    "如果类别间有一些自然顺序，\n",
    "比如说我们试图预测$\\{\\text{婴儿}, \\text{儿童}, \\text{青少年}, \\text{青年人}, \\text{中年人}, \\text{老年人}\\}$，\n",
    "那么将这个问题转变为回归问题，并且保留这种格式是有意义的。\n",
    "\n",
    "但是一般的分类问题并不与类别之间的自然顺序有关。\n",
    "幸运的是，统计学家很早以前就发明了一种表示分类数据的简单方法：*独热编码*（one-hot encoding）。\n",
    "独热编码是一个向量，它的分量和类别一样多。\n",
    "类别对应的分量设置为1，其他所有分量设置为0。\n",
    "在我们的例子中，标签$y$将是一个三维向量，\n",
    "其中$(1, 0, 0)$对应于“猫”、$(0, 1, 0)$对应于“鸡”、$(0, 0, 1)$对应于“狗”：\n",
    "\n",
    "$$y \\in \\{(1, 0, 0), (0, 1, 0), (0, 0, 1)\\}.$$\n",
    "\n",
    "## 网络架构\n",
    "\n",
    "对上面的问题，我们定义为：\n",
    "\n",
    "- 有四个输入特征（$2 \\times 2$ 的图像有 4 个像素）\n",
    "- 需要有三个仿射函数（affine function）对应 3 个输出（猫、鸡、狗）\n",
    "\n",
    "![figure](../assets/ch3_4_1.svg)\n",
    "\n",
    "这个时候，每个输出都会有一个权值向量和一个偏执，所以变成权值矩阵 $\\mathbf{W}$ 和偏置向量 $\\boldsymbol{b}$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\\\\n",
    "o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\\\\n",
    "o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "写为：\n",
    "\n",
    "$$\n",
    "\\mathbf{o} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "可以看出来，这也是一个全连接层。全连接层是“完全”连接的，可能有很多可学习的参数。\n",
    "具体来说，对于任何具有$d$个输入和$q$个输出的全连接层，\n",
    "参数开销为$\\mathcal{O}(dq)$，这个数字在实践中可能高得令人望而却步。\n",
    "幸运的是，将$d$个输入转换为$q$个输出的成本可以减少到$\\mathcal{O}(\\frac{dq}{n})$，\n",
    "其中超参数$n$可以由我们灵活指定，以在实际应用中平衡参数节约和模型有效性\n",
    "\n",
    "## 关于其他\n",
    "\n",
    "- Softmax 回归\n",
    "- 此回归的损失函数\n",
    "- 损失函数关于加权和的导数\n",
    "- etc. \n",
    "\n",
    "上面的内容看 Onenote\n",
    "\n",
    "然后关于信息熵的内容先跳过了，但是你记得回来看下"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "0559c7b697746d8ada537ecaf8430efdacd6ab4f7a5ef8d73b5846265590c7b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
